{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uOWsBZntZM9"
      },
      "source": [
        "## Montage du drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M4xk_xFV25l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\", force_remount=True)\n",
        "root = \"/content/gdrive/MyDrive/Stage_Bilbao_Neuroblastoma\"\n",
        "!unzip {root}/DataBase/250x250_split_BA.zip &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMkxRs8_Zoi"
      },
      "source": [
        "## Hyper-paramètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyHDwqFBtVTX"
      },
      "outputs": [],
      "source": [
        "backbone = \"ResNet\"\n",
        "version  = 152\n",
        "\n",
        "lr = 1e-4\n",
        "batch_size = 16\n",
        "optimizer_name = \"SGD\"\n",
        "\n",
        "# Do epochs iterations or reach acc_goal on training dataset\n",
        "# To only do epochs iterations, set train_acc_goal > 1\n",
        "epochs = 25\n",
        "train_acc_goal = 1.1\n",
        "\n",
        "# experiments saved in {backbone}{version}_{optimizer}_{tag}\n",
        "TAG = \"UFN10\"\n",
        "\n",
        "NUM_SESSION = 10\n",
        "BEST_LOSS = 0.278\n",
        "BEST_ACCU = 0.883"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBP1H2nAQfaN"
      },
      "source": [
        "## Imports python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoUVKA9APZEZ"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics &> /dev/null\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "import torchmetrics.classification as classification\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torchvision import models\n",
        "from google.colab import runtime\n",
        "\n",
        "from torchvision.models.vgg import vgg11, VGG11_Weights, vgg13, VGG13_Weights, vgg16, VGG16_Weights, vgg19, VGG19_Weights\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2hQ_2kYRcg9"
      },
      "source": [
        "## Chargement des datasets et preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9b76e0sRieL"
      },
      "outputs": [],
      "source": [
        "root_dir = \".\"\n",
        "train_dir = \"train/\"\n",
        "valid_dir = \"valid/\"\n",
        "test_dir  = \"test/\"\n",
        "\n",
        "if backbone == \"Inception\":\n",
        "    input_size = (299, 299)\n",
        "else:\n",
        "    input_size = (224, 224)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Resize(input_size, antialias=True),\n",
        "    transforms.RandomCrop(input_size),\n",
        "    transforms.Normalize((0.66747984797634830, 0.5799524696639141, 0.78054363559995920), (0.23162625605944703, 0.2340601507820534, 0.14160506754101998))\n",
        "    ]\n",
        ")\n",
        "eval_transform = transforms. Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Resize(input_size, antialias=True),\n",
        "    transforms.RandomCrop(input_size),\n",
        "    transforms.Normalize((0.66747984797634830, 0.5799524696639141, 0.78054363559995920), (0.23162625605944703, 0.2340601507820534, 0.14160506754101998))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chargement des données\n",
        "train_dataset = datasets.ImageFolder(f\"{root_dir}/{train_dir}\", train_transform)\n",
        "valid_dataset = datasets.ImageFolder(f\"{root_dir}/{valid_dir}\", eval_transform)\n",
        "\n",
        "print(len(train_dataset), len(valid_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH6iEdB-cl1h"
      },
      "source": [
        "## Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TG_LVwDcnlj"
      },
      "outputs": [],
      "source": [
        "def get_classifier(kind: str, in_features: int):\n",
        "    layers = []\n",
        "    if kind and kind.startswith(\"CNL\"):\n",
        "        nb_ReLU = int(kind[-1])\n",
        "        for i in range(nb_ReLU):\n",
        "            layers += [\n",
        "                nn.Linear(in_features >> i, in_features >> (i + 1), bias=True),\n",
        "                nn.LeakyReLU(0.2, inplace=False),\n",
        "                nn.Dropout(0.5, inplace=False)\n",
        "            ]\n",
        "        in_features = in_features >> (i+1)\n",
        "\n",
        "    layers += [\n",
        "        nn.Linear(in_features, 1, bias=True),\n",
        "        nn.Sigmoid()\n",
        "    ]\n",
        "    classifier = nn.Sequential(*layers)\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def print_trainable(model):\n",
        "    total_params, gradient_params = 0, 0\n",
        "    for param in model.parameters():\n",
        "        total_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            gradient_params += param.numel()\n",
        "    print(f\"{gradient_params}/{total_params} parameters will be train.\")\n",
        "\n",
        "\n",
        "class ImprovedInception(nn.Module):\n",
        "    def __init__(self, version=3):\n",
        "        super(ImprovedInception, self).__init__()\n",
        "        if version == 3:\n",
        "            self.inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "            raise ValueError(f\"Inception version {version} not available.\")\n",
        "\n",
        "        self.inception.fc = get_classifier(TAG, in_features=2048)\n",
        "\n",
        "    def fine_tune(self, unfrozen):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        layers = list(self.parameters())\n",
        "        num_layer = len(layers) - 1\n",
        "\n",
        "        while unfrozen and num_layer:\n",
        "            layer = layers[num_layer]\n",
        "            ndim  = len(layer.shape)\n",
        "            if ndim > 1:\n",
        "                unfrozen -= 1\n",
        "            layer.requires_grad = True\n",
        "            num_layer -= 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.inception(x)\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        return filter(lambda p: p.requires_grad, self.parameters())\n",
        "\n",
        "\n",
        "class ImprovedVGG(nn.Module):\n",
        "    def __init__(self, version=16):\n",
        "        super(ImprovedVGG, self).__init__()\n",
        "\n",
        "        if version == 11:\n",
        "            self.vgg = vgg11(weights=VGG11_Weights.IMAGENET1K_V1)\n",
        "        elif version == 13:\n",
        "            self.vgg = vgg13(weights=VGG13_Weights.IMAGENET1K_V1)\n",
        "        elif version == 16:\n",
        "            self.vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
        "        elif version == 19:\n",
        "            self.vgg = vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "            raise ValueError(f\"VGG version {version} does not exist.\")\n",
        "\n",
        "        self.vgg.classifier[6] = get_classifier(TAG, in_features=4096)\n",
        "\n",
        "    def fine_tune(self, unfrozen):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        layers = list(self.parameters())\n",
        "        num_layer = len(layers) - 1\n",
        "\n",
        "        while unfrozen and num_layer:\n",
        "            layer = layers[num_layer]\n",
        "            ndim  = len(layer.shape)\n",
        "            if ndim > 1:\n",
        "                unfrozen -= 1\n",
        "            layer.requires_grad = True\n",
        "            num_layer -= 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vgg(x)\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        return filter(lambda p: p.requires_grad, self.parameters())\n",
        "\n",
        "\n",
        "class ImprovedResNet(nn.Module):\n",
        "    def __init__(self, resnet_version=18):\n",
        "        super(ImprovedResNet, self).__init__()\n",
        "\n",
        "        if resnet_version == 18:\n",
        "            self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        elif resnet_version == 34:\n",
        "            self.resnet = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "        elif resnet_version == 50:\n",
        "            self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "        elif resnet_version == 101:\n",
        "            self.resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)\n",
        "        elif resnet_version == 152:\n",
        "            self.resnet = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)\n",
        "        else:\n",
        "            raise ValueError(f\"ResNet version {resnet_version} does not exist.\")\n",
        "\n",
        "        in_features = 512 if resnet_version <= 34 else 2048\n",
        "        self.resnet.fc = get_classifier(TAG, in_features)\n",
        "\n",
        "    def fine_tune(self, unfrozen):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        layers = list(self.parameters())\n",
        "        num_layer = len(layers) - 1\n",
        "\n",
        "        while unfrozen and num_layer:\n",
        "            layer = layers[num_layer]\n",
        "            ndim  = len(layer.shape)\n",
        "            if ndim > 1:\n",
        "                unfrozen -= 1\n",
        "            layer.requires_grad = True\n",
        "            num_layer -= 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        return filter(lambda p: p.requires_grad, self.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_INuU9dn_pc"
      },
      "source": [
        "## Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3hrn8t3qjq0"
      },
      "outputs": [],
      "source": [
        "from IPython.utils.path import target_outdated\n",
        "def train_model(model, loader, criterion, accuracy):\n",
        "    loss, accu = np.zeros((len(loader))), np.zeros((len(loader)))\n",
        "    accuracy.reset()\n",
        "    model.train()\n",
        "    for b, (inputs, labels) in enumerate(loader):\n",
        "        (inputs, labels) = (inputs.to(device), labels.to(device))\n",
        "        labels  = labels.unsqueeze(1).to(torch.float32)\n",
        "        outputs = model(inputs)\n",
        "        # outputs, _ = model(inputs)\n",
        "        cr_loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cr_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss[b] = cr_loss.item()\n",
        "        accu[b] = accuracy(outputs, labels).item()\n",
        "\n",
        "    mean_loss = np.mean(loss)\n",
        "    mean_accu = np.mean(accu)\n",
        "    return mean_loss, mean_accu\n",
        "\n",
        "def eval_model(model, loader, criterion, accuracy):\n",
        "    loss, accu = np.zeros((len(loader))), np.zeros((len(loader)))\n",
        "    accuracy.reset()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for b, (inputs, labels) in enumerate(loader):\n",
        "            (inputs, labels) = (inputs.to(device), labels.to(device))\n",
        "            labels  = labels.unsqueeze(1).to(torch.float32)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            cr_loss = criterion(outputs, labels)\n",
        "            loss[b] = cr_loss.item()\n",
        "            accu[b] = accuracy(outputs, labels).item()\n",
        "\n",
        "    mean_loss = np.mean(loss)\n",
        "    mean_accu = np.mean(accu)\n",
        "    return mean_loss, mean_accu\n",
        "\n",
        "def delete_file(path):\n",
        "    with open(path, \"w\") as fd:\n",
        "        fd.write(\"\")\n",
        "    os.remove(path)\n",
        "\n",
        "def find_backup_path(tab_name: str):\n",
        "    os.makedirs(tab_name, exist_ok=True)\n",
        "    backup_path = f\"{tab_name}/{lr}_{batch_size}\"\n",
        "    os.makedirs(backup_path, exist_ok=True)\n",
        "    backup_path += \"/\"\n",
        "    i = 1\n",
        "    while os.path.exists(backup_path + str(i)): i += 1\n",
        "    backup_path += str(i)\n",
        "    return backup_path\n",
        "\n",
        "def init_exp(backbone, version):\n",
        "    def is_tmp_folder(tmp_folder: str):\n",
        "        try:\n",
        "            os.makedirs(tmp_folder)\n",
        "            return False\n",
        "        except FileExistsError:\n",
        "            return True\n",
        "\n",
        "    best_metrics = {\"loss\": float(\"inf\"), \"accu\": 0}\n",
        "    if is_tmp_folder(TMP_PATH):\n",
        "        H = {\n",
        "            \"train\": {\"loss\": np.load(f\"{TMP_PATH}/train_loss.npy\"),\n",
        "                      \"accu\": np.load(f\"{TMP_PATH}/train_accu.npy\")},\n",
        "            \"valid\": {\"loss\": np.load(f\"{TMP_PATH}/valid_loss.npy\"),\n",
        "                      \"accu\": np.load(f\"{TMP_PATH}/valid_accu.npy\")}\n",
        "            }\n",
        "        best_metrics[\"loss\"] = np.min(H[\"valid\"][\"loss\"][np.nonzero(H[\"valid\"][\"loss\"])])\n",
        "        best_metrics[\"accu\"] = np.max(H[\"valid\"][\"accu\"])\n",
        "        weight_path = glob.glob(os.path.join(TMP_PATH, \"weight_*.pth\"))[0]\n",
        "        e = int(weight_path.split(\"_\")[-1].split(\".\")[0])\n",
        "        if backbone == \"ResNet\":\n",
        "            model = ImprovedResNet(resnet_version=version)\n",
        "        elif backbone == \"VGG\":\n",
        "            model = ImprovedVGG(version)\n",
        "        elif backbone == \"Inception\":\n",
        "            model = ImprovedInception(version)\n",
        "        else:\n",
        "            model = None\n",
        "        model.load_state_dict(torch.load(weight_path))\n",
        "        # remove residual .pth files\n",
        "        weight_files = glob.glob(f\"{TMP_PATH}/weight_*.pth\")\n",
        "        weight_files.remove(weight_path)\n",
        "        for w_file in weight_files:\n",
        "            delete_file(w_file)\n",
        "    else:\n",
        "        H = {\n",
        "            \"train\": {\"loss\": np.zeros((epochs+1)),\n",
        "                      \"accu\": np.zeros((epochs+1))},\n",
        "            \"valid\": {\"loss\": np.zeros((epochs+1)),\n",
        "                      \"accu\": np.zeros((epochs+1))}\n",
        "            }\n",
        "        e = 0\n",
        "        if backbone == \"ResNet\":\n",
        "            model = ImprovedResNet(resnet_version=version)\n",
        "        elif backbone == \"VGG\":\n",
        "            model = ImprovedVGG(version)\n",
        "        elif backbone == \"Inception\":\n",
        "            model = ImprovedInception(version)\n",
        "        else:\n",
        "            model = None\n",
        "    model.to(device)\n",
        "    if TAG and TAG.startswith(\"UFN\"):\n",
        "        unfrozen = int(TAG[-1]) + 1\n",
        "    else:\n",
        "        unfrozen = 1\n",
        "    model.fine_tune(unfrozen)\n",
        "    return H, best_metrics, e, model\n",
        "\n",
        "def save_tmp_exp(H, e, model):\n",
        "    os.makedirs(TMP_PATH, exist_ok=True)\n",
        "    np.save(f\"{TMP_PATH}/train_loss.npy\", H[\"train\"][\"loss\"])\n",
        "    np.save(f\"{TMP_PATH}/train_accu.npy\", H[\"train\"][\"accu\"])\n",
        "    np.save(f\"{TMP_PATH}/valid_loss.npy\", H[\"valid\"][\"loss\"])\n",
        "    np.save(f\"{TMP_PATH}/valid_accu.npy\", H[\"valid\"][\"accu\"])\n",
        "    torch.save(model.state_dict(), f\"{TMP_PATH}/weight_{e}.pth\")\n",
        "    if e > 1:\n",
        "      delete_file(f\"{TMP_PATH}/weight_{e-1}.pth\")\n",
        "\n",
        "def create_details_dic(train_loader, valid_loader, optimizer_name, lr, batch_size, H):\n",
        "    details = {}\n",
        "    details[\"loader\"]   = {\"train\": len(train_loader), \"valid\": len(valid_loader)}\n",
        "    details[\"learning\"] = {\"optimizer\": optimizer_name, \"lr\": lr, \"batch_size\": batch_size, \"epoch\": len(H[\"valid\"][\"loss\"])-1}\n",
        "    details[\"best\"] = {\"epoch_loss\": int(np.argmin(H[\"valid\"][\"loss\"])),\n",
        "                       \"epoch_accu\": int(np.argmax(H[\"valid\"][\"accu\"])),\n",
        "                       \"loss\": float(np.min(H[\"valid\"][\"loss\"])),\n",
        "                       \"accu\": float(np.max(H[\"valid\"][\"accu\"]))}\n",
        "    details[\"train_loss\"] = H[\"train\"][\"loss\"].tolist()\n",
        "    details[\"train_accu\"] = H[\"train\"][\"accu\"].tolist()\n",
        "    details[\"valid_loss\"] = H[\"valid\"][\"loss\"].tolist()\n",
        "    details[\"valid_accu\"] = H[\"valid\"][\"accu\"].tolist()\n",
        "    return details\n",
        "\n",
        "def make_backup_from_tmp(H, details):\n",
        "    backup_path = find_backup_path(TAB_NAME)\n",
        "    os.rename(TMP_PATH, backup_path)\n",
        "    with open(f\"{backup_path}/details.json\", \"w\") as fd:\n",
        "        json.dump(details, fd)\n",
        "    e = details[\"learning\"][\"epoch\"]\n",
        "    plot_metric(H, max_epoch=e, metric=\"accu\")\n",
        "    plt.savefig(f\"{backup_path}/accu.pdf\", bbox_inches=\"tight\")\n",
        "    plot_metric(H, max_epoch=e, metric=\"loss\")\n",
        "    plt.savefig(f\"{backup_path}/loss.pdf\", bbox_inches=\"tight\")\n",
        "    # remove useless files\n",
        "    for npy_file in [\"train_loss\", \"train_accu\", \"valid_loss\", \"valid_accu\"]:\n",
        "        file_ = f\"{backup_path}/{npy_file}.npy\"\n",
        "        delete_file(file_)\n",
        "    weight_files = glob.glob(f\"{backup_path}/weight_*.pth\")\n",
        "    for w_file in weight_files:\n",
        "        delete_file(w_file)\n",
        "\n",
        "def plot_metric(H, max_epoch:int, metric:str=\"accu\"):\n",
        "    fig = plt.figure(figsize=(6, 3))\n",
        "    cmap = plt.get_cmap(\"tab10\")\n",
        "    colors = [cmap(1), cmap(0)]\n",
        "    xline = np.arange(max_epoch+1)\n",
        "    handles = []\n",
        "    labels = [\"valid\", \"train\"]\n",
        "    for c, dataset in zip(colors, labels):\n",
        "        mean = H[dataset][f\"{metric}\"][:max_epoch+1]\n",
        "        line, = plt.plot(xline, mean, color=c)\n",
        "        handles += [line]\n",
        "    plt.xlabel(\"epoch\")\n",
        "    if metric == \"accu\":\n",
        "        plt.ylim([min(min(H[\"train\"][\"accu\"]), min(H[\"valid\"][\"accu\"])), 1])\n",
        "    elif metric == \"loss\":\n",
        "        plt.ylim([0, max(max(H[\"train\"][f\"loss\"]), max(H[\"valid\"][f\"loss\"]))])\n",
        "    plt.legend(handles[::-1], labels[::-1])\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZINkzqRQiuB"
      },
      "outputs": [],
      "source": [
        "TAB_NAME = f\"{root}/G_Collab/backup/{backbone}{version}_{optimizer_name}\"\n",
        "if TAG: TAB_NAME += f\"_{TAG}\"\n",
        "os.makedirs(TAB_NAME, exist_ok=True)\n",
        "os.makedirs(f\"{TAB_NAME}/__Summary__\", exist_ok=True)\n",
        "os.makedirs(f\"{TAB_NAME}/{lr}_{batch_size}\", exist_ok=True)\n",
        "TMP_PATH = f\"{TAB_NAME}/{lr}_{batch_size}/tmp\"\n",
        "\n",
        "remain_session = NUM_SESSION - len(os.listdir(f\"{TAB_NAME}/{lr}_{batch_size}\")) + os.path.exists(TMP_PATH)\n",
        "if TAG:\n",
        "    print(f\"Running {remain_session} sessions for {backbone}{version} (#{TAG})\")\n",
        "else:\n",
        "    print(f\"Running {remain_session} sessions for {backbone}{version}\")\n",
        "print(f\"optimized by {optimizer_name} with {lr=} and {batch_size=}.\")\n",
        "\n",
        "for _ in range(remain_session):\n",
        "    H, best_metrics, e0, model = init_exp(backbone, version)\n",
        "    best_valid_loss = best_metrics[\"loss\"]\n",
        "    best_valid_accu = best_metrics[\"accu\"]\n",
        "    best_epoch_loss, best_epoch_accu = -1, -1\n",
        "\n",
        "    if optimizer_name == \"SGD\":\n",
        "        optimizer = optim.SGD(model.get_trainable_parameters(), lr, momentum=0.9)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.get_trainable_parameters(), lr, betas=(0.9, 0.999))\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    accuracy  = classification.BinaryAccuracy().to(device)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size, shuffle=False)\n",
        "\n",
        "    # Epoch 0\n",
        "    if e0 == 0:\n",
        "        (H[\"train\"][\"loss\"][0], H[\"train\"][\"accu\"][0]) = eval_model(model, train_loader, criterion, accuracy)\n",
        "        (H[\"valid\"][\"loss\"][0], H[\"valid\"][\"accu\"][0]) = eval_model(model, valid_loader, criterion, accuracy)\n",
        "\n",
        "    for e in tqdm(range(e0+1, epochs+1)):\n",
        "        (H[\"train\"][\"loss\"][e], H[\"train\"][\"accu\"][e]) = train_model(model, train_loader, criterion, accuracy)\n",
        "        (H[\"valid\"][\"loss\"][e], H[\"valid\"][\"accu\"][e]) = eval_model(model, valid_loader, criterion, accuracy)\n",
        "\n",
        "        if best_valid_loss > H[\"valid\"][\"loss\"][e]:\n",
        "            best_valid_loss = H[\"valid\"][\"loss\"][e]\n",
        "            best_epoch_loss = e\n",
        "            if BEST_LOSS > best_valid_loss:\n",
        "                torch.save(model.state_dict(), f\"{TMP_PATH}/model_loss.pth\")\n",
        "                BEST_LOSS = best_valid_loss\n",
        "        if best_valid_accu < H[\"valid\"][\"accu\"][e]:\n",
        "            best_valid_accu = H[\"valid\"][\"accu\"][e]\n",
        "            best_epoch_accu = e\n",
        "            if BEST_ACCU < best_valid_accu:\n",
        "                torch.save(model.state_dict(), f\"{TMP_PATH}/model_accu.pth\")\n",
        "                BEST_ACCU = best_valid_accu\n",
        "        save_tmp_exp(H, e, model)\n",
        "        if H[\"train\"][\"accu\"][e] > train_acc_goal:\n",
        "            H[\"train\"][\"loss\"][e+1:] = None\n",
        "            H[\"train\"][\"accu\"][e+1:] = None\n",
        "            H[\"valid\"][\"loss\"][e+1:] = None\n",
        "            H[\"valid\"][\"accu\"][e+1:] = None\n",
        "            break\n",
        "\n",
        "    details = create_details_dic(train_loader, valid_loader, optimizer_name, lr, batch_size, H)\n",
        "    make_backup_from_tmp(H, details)\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}